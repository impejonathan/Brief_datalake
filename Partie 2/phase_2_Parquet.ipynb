{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# le get et post ( Recup de secret dans coffre fort \"keyvolts\")  \n",
    "# PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import des biblioth√®ques n√©cessaires\n",
    "from azure.identity import ClientSecretCredential, DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "\n",
    "from datasets import load_dataset\n",
    "import tempfile\n",
    "\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from azure.core.exceptions import AzureError\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Chargement des variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "# 2. Configuration des informations d'authentification\n",
    "# Configuration Key Vault\n",
    "key_vault_url = os.getenv('KEY_VAULT_URL')\n",
    "secret_name = os.getenv('WRITE_SECRET_NAME')\n",
    "\n",
    "# Configuration Azure AD\n",
    "tenant_id = os.getenv('TENANT_ID')\n",
    "client_id = os.getenv('WRITE_CLIENT_ID')\n",
    "\n",
    "# Configuration Storage\n",
    "storage_account_name = os.getenv('STORAGE_ACCOUNT_NAME')\n",
    "container_name = os.getenv('CONTAINER_NAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. R√©cup√©ration du secret depuis Key Vault\n",
    "def get_secret_from_keyvault():\n",
    "    try:\n",
    "        # Utilisation de DefaultAzureCredential pour l'authentification\n",
    "        credential = DefaultAzureCredential()\n",
    "        \n",
    "        # Cr√©ation du client Key Vault\n",
    "        secret_client = SecretClient(vault_url=key_vault_url, credential=credential)\n",
    "        \n",
    "        # R√©cup√©ration du secret\n",
    "        secret = secret_client.get_secret(secret_name)\n",
    "        return secret.value\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la r√©cup√©ration du secret: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_credentials():\n",
    "    client_secret = get_secret_from_keyvault()\n",
    "    if client_secret:\n",
    "        return ClientSecretCredential(\n",
    "            tenant_id=tenant_id,\n",
    "            client_id=client_id,\n",
    "            client_secret=client_secret\n",
    "        )\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Fonction pour uploader le fichier vers Data Lake\n",
    "def upload_to_datalake(local_file_path, destination_path):\n",
    "    try:\n",
    "        # Obtention des credentials\n",
    "        credential = create_credentials()\n",
    "        if not credential:\n",
    "            raise Exception(\"Impossible d'obtenir les credentials\")\n",
    "\n",
    "        # URL du service\n",
    "        account_url = f\"https://{storage_account_name}.dfs.core.windows.net\"\n",
    "        \n",
    "        # Cr√©ation du client service\n",
    "        service_client = DataLakeServiceClient(account_url, credential=credential)\n",
    "        \n",
    "        # Obtention du client syst√®me de fichiers\n",
    "        file_system_client = service_client.get_file_system_client(file_system=container_name)\n",
    "        \n",
    "        # Cr√©ation du client fichier\n",
    "        file_client = file_system_client.create_file(destination_path)\n",
    "        \n",
    "        # Lecture et upload du fichier\n",
    "        with open(local_file_path, 'rb') as local_file:\n",
    "            file_contents = local_file.read()\n",
    "            \n",
    "        file_client.append_data(data=file_contents, offset=0, length=len(file_contents))\n",
    "        file_client.flush_data(len(file_contents))\n",
    "            \n",
    "        print(f\"Fichier upload√© avec succ√®s vers: {destination_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur est survenue: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction pour t√©l√©charger et uploader les fichiers Parquet depuis Hugging Face\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Niveau 1 avec 1 fichier \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def process_single_parquet_file():\n",
    "#     try:\n",
    "#         # URL du fichier\n",
    "#         file_url = \"https://huggingface.co/datasets/Marqo/amazon-products-eval/resolve/main/data/data-00000-of-00105.parquet\"\n",
    "        \n",
    "#         print(\"üì• D√©but du transfert du fichier vers Data Lake...\")\n",
    "        \n",
    "#         # Configuration de la requ√™te\n",
    "#         headers = {\n",
    "#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "#         }\n",
    "        \n",
    "#         # Initialisation de la requ√™te en streaming\n",
    "#         response = requests.get(file_url, headers=headers, stream=True)\n",
    "#         response.raise_for_status()\n",
    "        \n",
    "#         # Obtention de la taille totale du fichier\n",
    "#         total_size = int(response.headers.get('content-length', 0))\n",
    "        \n",
    "#         # Configuration du client Data Lake\n",
    "#         credential = create_credentials()\n",
    "#         if not credential:\n",
    "#             raise Exception(\"Impossible d'obtenir les credentials\")\n",
    "\n",
    "#         account_url = f\"https://{storage_account_name}.dfs.core.windows.net\"\n",
    "#         service_client = DataLakeServiceClient(account_url, credential=credential)\n",
    "#         file_system_client = service_client.get_file_system_client(container_name)\n",
    "        \n",
    "#         # Cr√©ation du fichier dans le Data Lake\n",
    "#         destination_path = \"huggingface_data/data-00000-of-00105.parquet\"\n",
    "#         file_client = file_system_client.create_file(destination_path)\n",
    "        \n",
    "#         # Transfert direct des donn√©es\n",
    "#         current_position = 0\n",
    "#         for chunk in response.iter_content(chunk_size=4*1024*1024):  # 4MB chunks\n",
    "#             if chunk:\n",
    "#                 file_client.append_data(data=chunk, offset=current_position)\n",
    "#                 current_position += len(chunk)\n",
    "                \n",
    "#                 # Affichage de la progression\n",
    "#                 if total_size:\n",
    "#                     progress = (current_position / total_size) * 100\n",
    "#                     print(f\"\\rProgression du transfert: {progress:.2f}% ({current_position/(1024*1024):.2f} Mo / {total_size/(1024*1024):.2f} Mo)\", end='')\n",
    "\n",
    "#         # Finalisation du fichier\n",
    "#         file_client.flush_data(current_position)\n",
    "#         print(\"\\n‚úÖ Transfert termin√© avec succ√®s!\")\n",
    "        \n",
    "#         print(f\"Taille totale transf√©r√©e: {current_position/(1024*1024):.2f} Mo\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n‚ùå Une erreur est survenue: {str(e)}\")\n",
    "\n",
    "# def verify_uploaded_files():\n",
    "#     try:\n",
    "#         credential = create_credentials()\n",
    "#         if not credential:\n",
    "#             raise Exception(\"Impossible d'obtenir les credentials\")\n",
    "\n",
    "#         account_url = f\"https://{storage_account_name}.dfs.core.windows.net\"\n",
    "#         service_client = DataLakeServiceClient(account_url, credential=credential)\n",
    "#         file_system_client = service_client.get_file_system_client(container_name)\n",
    "        \n",
    "#         print(\"\\nüìã Fichiers upload√©s dans le Data Lake:\")\n",
    "#         print(\"-\" * 50)\n",
    "        \n",
    "#         paths = file_system_client.get_paths(path=\"huggingface_data\")\n",
    "#         for path in paths:\n",
    "#             size_mb = path.content_length / (1024 * 1024)\n",
    "#             print(f\"üìÑ {path.name} ({size_mb:.2f} Mo)\")\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Erreur lors de la v√©rification: {str(e)}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"üöÄ D√©marrage du traitement...\")\n",
    "#     process_single_parquet_file()\n",
    "#     verify_uploaded_files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIVEAU 2 -- Mode Batch --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Configuration du batch:\n",
      "- Fichiers 0 √† 4\n",
      "- 3 t√©l√©chargements simultan√©s\n",
      "üöÄ D√©marrage du transfert batch (5 fichiers)...\n",
      "\n",
      "üì• D√©but du transfert du fichier data-00000-of-00105.parquet...\n",
      "üì• D√©but du transfert du fichier data-00001-of-00105.parquet...\n",
      "\n",
      "\n",
      "üì• D√©but du transfert du fichier data-00002-of-00105.parquet...\n",
      "data-00000-of-00105.parquet: 100.00% (439.64 Mo / 439.64 Mo)\n",
      "‚úÖ data-00000-of-00105.parquet transf√©r√© avec succ√®s! (439.64 Mo)\n",
      "\n",
      "üì• D√©but du transfert du fichier data-00003-of-00105.parquet...\n",
      "data-00002-of-00105.parquet: 100.00% (447.48 Mo / 447.48 Mo)\n",
      "‚úÖ data-00002-of-00105.parquet transf√©r√© avec succ√®s! (447.48 Mo)\n",
      "\n",
      "üì• D√©but du transfert du fichier data-00004-of-00105.parquet...\n",
      "data-00001-of-00105.parquet: 100.00% (448.80 Mo / 448.80 Mo)\n",
      "‚úÖ data-00001-of-00105.parquet transf√©r√© avec succ√®s! (448.80 Mo)\n",
      "data-00003-of-00105.parquet: 36.05% (164.00 Mo / 454.94 Mo)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def transfer_single_file(file_number, base_url, file_system_client):\n",
    "    \"\"\"Fonction pour transf√©rer un seul fichier\"\"\"\n",
    "    try:\n",
    "        file_name = f\"data-{str(file_number).zfill(5)}-of-00105.parquet\"\n",
    "        file_url = f\"{base_url}{file_name}\"\n",
    "        destination_path = f\"huggingface_data/{file_name}\"\n",
    "        \n",
    "        print(f\"\\nüì• D√©but du transfert du fichier {file_name}...\")\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(file_url, headers=headers, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        \n",
    "        file_client = file_system_client.create_file(destination_path)\n",
    "        \n",
    "        current_position = 0\n",
    "        for chunk in response.iter_content(chunk_size=4*1024*1024):\n",
    "            if chunk:\n",
    "                file_client.append_data(data=chunk, offset=current_position)\n",
    "                current_position += len(chunk)\n",
    "                \n",
    "                if total_size:\n",
    "                    progress = (current_position / total_size) * 100\n",
    "                    print(f\"\\r{file_name}: {progress:.2f}% ({current_position/(1024*1024):.2f} Mo / {total_size/(1024*1024):.2f} Mo)\", end='')\n",
    "\n",
    "        file_client.flush_data(current_position)\n",
    "        print(f\"\\n‚úÖ {file_name} transf√©r√© avec succ√®s! ({current_position/(1024*1024):.2f} Mo)\")\n",
    "        return True, file_name, current_position\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Erreur lors du transfert de {file_name}: {str(e)}\")\n",
    "        return False, file_name, 0\n",
    "\n",
    "def process_multiple_files(start_file=0, num_files=5, max_workers=3):\n",
    "    \"\"\"Fonction principale pour traiter plusieurs fichiers en parall√®le\"\"\"\n",
    "    try:\n",
    "        print(f\"üöÄ D√©marrage du transfert batch ({num_files} fichiers)...\")\n",
    "        \n",
    "        base_url = \"https://huggingface.co/datasets/Marqo/amazon-products-eval/resolve/main/data/\"\n",
    "        \n",
    "        # Configuration du client Data Lake\n",
    "        credential = create_credentials()\n",
    "        if not credential:\n",
    "            raise Exception(\"Impossible d'obtenir les credentials\")\n",
    "\n",
    "        account_url = f\"https://{storage_account_name}.dfs.core.windows.net\"\n",
    "        service_client = DataLakeServiceClient(account_url, credential=credential)\n",
    "        file_system_client = service_client.get_file_system_client(container_name)\n",
    "        \n",
    "        # Traitement parall√®le des fichiers\n",
    "        successful_transfers = 0\n",
    "        total_size = 0\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_file = {\n",
    "                executor.submit(\n",
    "                    transfer_single_file, \n",
    "                    file_num, \n",
    "                    base_url, \n",
    "                    file_system_client\n",
    "                ): file_num \n",
    "                for file_num in range(start_file, start_file + num_files)\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(future_to_file):\n",
    "                success, file_name, size = future.result()\n",
    "                if success:\n",
    "                    successful_transfers += 1\n",
    "                    total_size += size\n",
    "\n",
    "        print(f\"\\nüìä R√©sum√© du transfert batch:\")\n",
    "        print(f\"- Fichiers transf√©r√©s avec succ√®s: {successful_transfers}/{num_files}\")\n",
    "        print(f\"- Taille totale transf√©r√©e: {total_size/(1024*1024*1024):.2f} Go\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur g√©n√©rale: {str(e)}\")\n",
    "\n",
    "def verify_uploaded_files():\n",
    "    \"\"\"Fonction pour v√©rifier les fichiers upload√©s\"\"\"\n",
    "    try:\n",
    "        credential = create_credentials()\n",
    "        if not credential:\n",
    "            raise Exception(\"Impossible d'obtenir les credentials\")\n",
    "\n",
    "        account_url = f\"https://{storage_account_name}.dfs.core.windows.net\"\n",
    "        service_client = DataLakeServiceClient(account_url, credential=credential)\n",
    "        file_system_client = service_client.get_file_system_client(container_name)\n",
    "        \n",
    "        print(\"\\nüìã Fichiers dans le Data Lake:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        total_size = 0\n",
    "        file_count = 0\n",
    "        \n",
    "        paths = file_system_client.get_paths(path=\"huggingface_data\")\n",
    "        for path in paths:\n",
    "            size_mb = path.content_length / (1024 * 1024)\n",
    "            print(f\"üìÑ {path.name} ({size_mb:.2f} Mo)\")\n",
    "            total_size += path.content_length\n",
    "            file_count += 1\n",
    "            \n",
    "        print(\"\\nüìä R√©sum√©:\")\n",
    "        print(f\"- Nombre total de fichiers: {file_count}\")\n",
    "        print(f\"- Taille totale: {total_size/(1024*1024*1024):.2f} Go\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de la v√©rification: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration du batch\n",
    "    START_FILE = 0       # Num√©ro du premier fichier\n",
    "    NUM_FILES = 5        # Nombre de fichiers √† traiter\n",
    "    MAX_WORKERS = 3      # Nombre de t√©l√©chargements simultan√©s\n",
    "    \n",
    "    print(\"üéØ Configuration du batch:\")\n",
    "    print(f\"- Fichiers {START_FILE} √† {START_FILE + NUM_FILES - 1}\")\n",
    "    print(f\"- {MAX_WORKERS} t√©l√©chargements simultan√©s\")\n",
    "    \n",
    "    # Ex√©cution du transfert batch\n",
    "    process_multiple_files(START_FILE, NUM_FILES, MAX_WORKERS)\n",
    "    \n",
    "    # V√©rification finale\n",
    "    verify_uploaded_files()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
